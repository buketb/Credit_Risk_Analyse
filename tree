
#Gain calculation in a tree
#Gain = gini_root - (prop(cases left leaf) * gini_left) - (prop(cases right leaf * gini_right))

# The Gini-measure of the root node is given below
gini_root <- 2 * 89 / 500 * 411 / 500

# Compute the Gini measure for the left leaf node
gini_ll <- 2* 45/446* 401/446

# Compute the Gini measure for the right leaf node
gini_rl <- 2 * 10/54 * 44/54

# Compute the gain
gain <- gini_root - 446 / 500 * gini_ll - 54 / 500 * gini_rl

# compare the gain-column in small_tree$splits with our computed gain, multiplied by 500, and assure they are the same
small_tree$splits
improve <- gain * 500


to overcome the imbalance. A first option is either oversampling your underrepresented group (in this case, the defaults) or 
#undersampling the overrepresented group (or non-defaults). Balancing the training set will have a positive effect on the 
#accuracy, and generally lead to better results. Note that over- or undersampling should only be 
#applied to the training set and not to the test set! 

#A second option is changing the prior probabilities in the rpart() function. 
#By default, the prior probabilities of default versus non-default are set equal to their proportions in the training set. 
#By making the prior probabilities for default bigger, you kind of trick R into attaching more importance to defaults, 
#leading to a better decision tree. 

#As a third option, the rpart() function allows the specification of a loss matrix. 
#In this loss matrix, different costs can be associated with the misclassification of a default as a non-default verses 
#the misclassification of a non-default as a default. By increasing the misclassification cost of the former, again, more 
#attention is drawn to the correct classification of defaults, improving the quality of the decision tree. As all three methods are 
#intended to overcome the problem of class imbalance, validation is very important. Where under- or oversampling may work 
#very well for some data sets, it might perform poorly for others, and the same could be true for the two other methods



#decision tree

fit_default <- rpart(loan_status ~ ., method= "class", data = "training_set)

plot(fit_default)



#which is the complexity parameter, is the threshold value for a decrease in overall lack of fit for any split. 
#If cp is not met, further splits will no longer be pursued. cp's default value is 0.01, but for complex problems, 
#it is advised to relax cp


#Include rpart.control to relax the complexity parameter to 0.001.
tree_undersample <- rpart(loan_status ~ ., method = "class",
                          data =  undersampled_training_set, 
                          control=rpart.control(cp=0.001))

# Plot the decision tree
plot(tree_undersample,uniform=T, main="Tree")

# Add labels to the decision tree
text(tree_undersample)

# Change the code below such that a tree is constructed with adjusted prior probabilities.
tree_prior <- rpart(loan_status ~ ., method = "class",
                    data = training_set, parms = list(prior = c(0.7, 0.3)),
                    control = rpart.control(cp = 0.001))

# Plot the decision tree
plot(tree_prior, uniform = TRUE)

# Add labels to the decision tree
text(tree_prior)
