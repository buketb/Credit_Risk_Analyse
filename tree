
#Gain calculation in a tree
#Gain = gini_root - (prop(cases left leaf) * gini_left) - (prop(cases right leaf * gini_right))

# The Gini-measure of the root node is given below
gini_root <- 2 * 89 / 500 * 411 / 500

# Compute the Gini measure for the left leaf node
gini_ll <- 2* 45/446* 401/446

# Compute the Gini measure for the right leaf node
gini_rl <- 2 * 10/54 * 44/54

# Compute the gain
gain <- gini_root - 446 / 500 * gini_ll - 54 / 500 * gini_rl

# compare the gain-column in small_tree$splits with our computed gain, multiplied by 500, and assure they are the same
small_tree$splits
improve <- gain * 500


to overcome the imbalance. A first option is either oversampling your underrepresented group (in this case, the defaults) or 
#undersampling the overrepresented group (or non-defaults). Balancing the training set will have a positive effect on the 
#accuracy, and generally lead to better results. Note that over- or undersampling should only be 
#applied to the training set and not to the test set! 

#A second option is changing the prior probabilities in the rpart() function. 
#By default, the prior probabilities of default versus non-default are set equal to their proportions in the training set. 
#By making the prior probabilities for default bigger, you kind of trick R into attaching more importance to defaults, 
#leading to a better decision tree. 

#As a third option, the rpart() function allows the specification of a loss matrix. 
#In this loss matrix, different costs can be associated with the misclassification of a default as a non-default verses 
#the misclassification of a non-default as a default. By increasing the misclassification cost of the former, again, more 
#attention is drawn to the correct classification of defaults, improving the quality of the decision tree. As all three methods are 
#intended to overcome the problem of class imbalance, validation is very important. Where under- or oversampling may work 
#very well for some data sets, it might perform poorly for others, and the same could be true for the two other methods



#decision tree

fit_default <- rpart(loan_status ~ ., method= "class", data = "training_set)

plot(fit_default)



#which is the complexity parameter, is the threshold value for a decrease in overall lack of fit for any split. 
#If cp is not met, further splits will no longer be pursued. cp's default value is 0.01, but for complex problems, 
#it is advised to relax cp


#Include rpart.control to relax the complexity parameter to 0.001.
tree_undersample <- rpart(loan_status ~ ., method = "class",
                          data =  undersampled_training_set, 
                          control=rpart.control(cp=0.001))

# Plot the decision tree
plot(tree_undersample,uniform=T, main="Tree")

# Add labels to the decision tree
text(tree_undersample)

# Change the code below such that a tree is constructed with adjusted prior probabilities.
tree_prior <- rpart(loan_status ~ ., method = "class",
                    data = training_set, parms = list(prior = c(0.7, 0.3)),
                    control = rpart.control(cp = 0.001))

# Plot the decision tree
plot(tree_prior, uniform = TRUE)

# Add labels to the decision tree
text(tree_prior)


# Change the code provided in the video such that a decision tree is constructed using a loss matrix penalizing 10 times more heavily for misclassified defaults.
tree_loss_matrix  <- rpart(loan_status ~ ., method = "class", data = training_set,
                           parms = list(loss = matrix(c(0, 10, 1, 0), ncol = 2)),
                           control = rpart.control(cp = 0.001))

# Plot the decision tree
plot(tree_loss_matrix, uniform = TRUE)

# Add labels to the decision tree
text(tree_loss_matrix)


#Too complex
#Overfitting


printcp(tree_undersample)

plotcp(tree_undersample) 
#this graph shows the minimum cross-validation error point as cp

#prune the tree

ptree_undersample <- prune(tree_undersample, cp= 0.00365)
plot(ptree_undersample, uniform=T)
text(ptree_undersample)

text(ptree_undersample, use.n =T) #to see the ratios

#more intuitive tree
library(rpart.plot)
prp(ptree_undersample)

prp(ptree_undersample, extra =1) # to see the ratios (defaults to non-defaults cases)

# tree_prior is loaded in your workspace

# Plot the cross-validated error rate as a function of the complexity parameter
plotcp(tree_prior)

# Use printcp() to identify for which complexity parameter the cross-validated error rate is minimized
printcp(tree_prior)

# Create an index for of the row with the minimum xerror
index <- which.min(tree_prior$cptable[, "xerror"])

# Create tree_min
tree_min <- tree_prior$cptable[index, "CP"]

#  Prune the tree using tree_min
ptree_prior <- prune(tree_prior, cp = tree_min)

# Use prp() to plot the pruned tree
prp(ptree_prior)


# set a seed and run the code to construct the tree with the loss matrix again
set.seed(345)
tree_loss_matrix  <- rpart(loan_status ~ ., method = "class", data = training_set,
                           parms = list(loss=matrix(c(0, 10, 1, 0), ncol = 2)),
                           control = rpart.control(cp = 0.001))

# Plot the cross-validated error rate as a function of the complexity parameter
plotcp(tree_loss_matrix)

# Prune the tree using cp = 0.0012788
ptree_loss_matrix <- prune(tree_loss_matrix, cp = 0.0012788)

# Use prp() and argument extra = 1 to plot the pruned tree
prp(ptree_loss_matrix, extra = 1)


# set a seed and run the code to obtain a tree using weights, minsplit and minbucket
set.seed(345)
tree_weights <- rpart(loan_status ~ ., method = "class",
                      data = training_set, weights = case_weights,
                      control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.001))

# Plot the cross-validated error rate for a changing cp
plotcp(tree_weights)

# Create an index for of the row with the minimum xerror
index <- which.min(tree_weights$cp[ , "xerror"])

# Create tree_min
tree_min <- tree_weights$cp[index, "CP"]

#  Prune the tree using tree_min
ptree_weights <- prune(tree_weights, tree_min)

# Plot the pruned tree using the rpart.plot()-package
prp(ptree_weights, extra = 1)


# Make predictions for each of the pruned trees using the test set.
pred_undersample <- predict(ptree_undersample, newdata = test_set,  type = "class")
pred_prior <- predict(ptree_prior, newdata = test_set, type = "class")
pred_loss_matrix <- predict(ptree_loss_matrix, newdata = test_set, type = "class")
pred_weights <- predict(ptree_weights, newdata = test_set, type = "class")

# Construct confusion matrices using the predictions.
confmat_undersample <- table(test_set$loan_status, pred_undersample)
confmat_prior <- table(test_set$loan_status, pred_prior)
confmat_loss_matrix <- table(test_set$loan_status, pred_loss_matrix)
confmat_weights <- table(test_set$loan_status, pred_weights)

# Compute the accuracies
acc_undersample <- sum(diag(confmat_undersample)) / nrow(test_set)
acc_prior <- sum(diag(confmat_prior)) / nrow(test_set)
acc_loss_matrix <- sum(diag(confmat_loss_matrix)) / nrow(test_set)
acc_weights <- sum(diag(confmat_weights)) / nrow(test_set)

#finding the right cut-off

log_model_full <-glm(loan_status ~., family="binomial", data =training_set)

predictions_all_full <- predict(log_reg_model, newdata = test_set, type ="response")

cutoff <- quantile(predictions_all_full, 0.8)
cutoff


pred_full_20 <-  ifelse(predictions_all_full > cutoff, 1,0)


true_and_predval <- cbind(test_set$loan_status, pred_full_20)
true_and_predval

accepted_loans <- pred_and_trueval[pred_full_20 == 0,1]
bad_rate <- sum(accepted_loans) /length(accepted_loans)
bad_rate



# Make predictions for the probability of default using the pruned tree and the test set.
prob_default_prior <- predict(ptree_prior, newdata = test_set)[ ,2]

# Obtain the cutoff for acceptance rate 80%
cutoff_prior <- quantile(prob_default_prior, 0.8)
  
# Obtain the binary predictions.
bin_pred_prior_80 <- ifelse(prob_default_prior > cutoff_prior, 1, 0)

# Obtain the actual default status for the accepted loans
accepted_status_prior_80 <- test_set$loan_status[bin_pred_prior_80 == 0]

# Obtain the bad rate for the accepted loans
sum(accepted_status_prior_80) / length(accepted_status_prior_80)

